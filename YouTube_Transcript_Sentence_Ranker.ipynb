{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Programming projects\\Python\\Extractive Retrival Summarization\\py-venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from typing import List, Tuple, Dict\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('punkt', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download the 'punkt_tab' data for sentence tokenization\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "nltk.download('punkt', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranscriptRanker:\n",
    "    def __init__(self, model_name: str = 'all-mpnet-base-v2'):\n",
    "        \"\"\"\n",
    "        Initialize the transcript ranker with the specified sentence transformer model.\n",
    "\n",
    "        Args:\n",
    "            model_name: The name of the SentenceTransformer model to use\n",
    "        \"\"\"\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.index = None\n",
    "        self.sentences = []\n",
    "        self.embeddings = None\n",
    "\n",
    "    def prepare_transcript(self, transcript_text: str) -> None:\n",
    "        \"\"\"\n",
    "        Process the transcript text by splitting it into sentences and creating embeddings.\n",
    "\n",
    "        Args:\n",
    "            transcript_text: The raw transcript text from YouTube\n",
    "        \"\"\"\n",
    "        # Split transcript into sentences\n",
    "        self.sentences = sent_tokenize(transcript_text)\n",
    "\n",
    "        # Create embeddings for all sentences\n",
    "        self.embeddings = self.model.encode(self.sentences, convert_to_tensor=True)\n",
    "\n",
    "        # Convert embeddings to numpy array for FAISS\n",
    "        embeddings_np = self.embeddings.cpu().numpy().astype(np.float32)\n",
    "\n",
    "        # Create and populate the FAISS index\n",
    "        dimension = embeddings_np.shape[1]\n",
    "        self.index = faiss.IndexFlatIP(dimension)  # Inner product similarity (equivalent to cosine for normalized vectors)\n",
    "        faiss.normalize_L2(embeddings_np)  # Normalize vectors for cosine similarity\n",
    "        self.index.add(embeddings_np)\n",
    "\n",
    "    def query(self, user_query: str, top_k: int = 5, diversity_factor: float = 0.5) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Find the most relevant sentences in the transcript for the given query.\n",
    "\n",
    "        Args:\n",
    "            user_query: The user's query\n",
    "            top_k: Number of top results to return\n",
    "            diversity_factor: Factor for MMR diversity (0-1, higher means more diversity)\n",
    "\n",
    "        Returns:\n",
    "            List of dictionaries containing the relevant sentences with scores and positions\n",
    "        \"\"\"\n",
    "        # Encode the query\n",
    "        query_embedding = self.model.encode(user_query, convert_to_tensor=True)\n",
    "        query_embedding_np = query_embedding.cpu().numpy().astype(np.float32).reshape(1, -1)\n",
    "        faiss.normalize_L2(query_embedding_np)\n",
    "\n",
    "        # Perform the search\n",
    "        D, I = self.index.search(query_embedding_np, len(self.sentences))\n",
    "\n",
    "        # Apply Maximal Marginal Relevance to diversify results\n",
    "        selected_indices = self._mmr(query_embedding, I[0], top_k, diversity_factor)\n",
    "\n",
    "        # Prepare results\n",
    "        results = []\n",
    "        for i, idx in enumerate(selected_indices):\n",
    "            results.append({\n",
    "                'sentence': self.sentences[idx],\n",
    "                'score': float(D[0][i]),  # Convert from numpy float to Python float\n",
    "                'position': idx\n",
    "            })\n",
    "\n",
    "        return results\n",
    "\n",
    "    def _mmr(self, query_embedding: torch.Tensor, initial_indices: List[int], top_k: int,\n",
    "             diversity_factor: float) -> List[int]:\n",
    "        \"\"\"\n",
    "        Apply Maximal Marginal Relevance to diversify results.\n",
    "\n",
    "        Args:\n",
    "            query_embedding: The embedding of the query\n",
    "            initial_indices: The initial indices from FAISS\n",
    "            top_k: Number of results to return\n",
    "            diversity_factor: Factor for diversity (0-1, higher means more diversity)\n",
    "\n",
    "        Returns:\n",
    "            List of indices selected by MMR\n",
    "        \"\"\"\n",
    "        query_embedding_np = query_embedding.cpu().numpy()\n",
    "\n",
    "        # Initialize\n",
    "        selected_indices = []\n",
    "        remaining_indices = initial_indices.tolist()\n",
    "\n",
    "        # Select first index (most similar to query)\n",
    "        selected_indices.append(remaining_indices[0])\n",
    "        remaining_indices.remove(remaining_indices[0])\n",
    "\n",
    "        # Select remaining indices using MMR\n",
    "        while len(selected_indices) < top_k and remaining_indices:\n",
    "            best_score = -np.inf\n",
    "            best_idx = -1\n",
    "\n",
    "            for idx in remaining_indices:\n",
    "                # Similarity to query\n",
    "                similarity_to_query = float(cosine_similarity(\n",
    "                    [query_embedding_np],\n",
    "                    [self.embeddings[idx].cpu().numpy()]\n",
    "                )[0][0])\n",
    "\n",
    "                # Maximum similarity to already selected sentences\n",
    "                max_similarity_to_selected = 0\n",
    "                for selected_idx in selected_indices:\n",
    "                    similarity = float(cosine_similarity(\n",
    "                        [self.embeddings[idx].cpu().numpy()],\n",
    "                        [self.embeddings[selected_idx].cpu().numpy()]\n",
    "                    )[0][0])\n",
    "                    max_similarity_to_selected = max(max_similarity_to_selected, similarity)\n",
    "\n",
    "                # Calculate MMR score\n",
    "                mmr_score = diversity_factor * similarity_to_query - (1 - diversity_factor) * max_similarity_to_selected\n",
    "\n",
    "                if mmr_score > best_score:\n",
    "                    best_score = mmr_score\n",
    "                    best_idx = idx\n",
    "\n",
    "            if best_idx != -1:\n",
    "                selected_indices.append(best_idx)\n",
    "                remaining_indices.remove(best_idx)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        return selected_indices\n",
    "\n",
    "    def highlight_transcript(self, ranked_results: List[Dict],\n",
    "                             context_sentences: int = 1) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Create a highlighted version of the transcript with context around the top hits.\n",
    "\n",
    "        Args:\n",
    "            ranked_results: The results from the query method\n",
    "            context_sentences: Number of sentences to include before and after the matched sentence\n",
    "\n",
    "        Returns:\n",
    "            List of transcript segments with highlighted sentences and context\n",
    "        \"\"\"\n",
    "        highlights = []\n",
    "\n",
    "        for result in ranked_results:\n",
    "            position = result['position']\n",
    "            start_pos = max(0, position - context_sentences)\n",
    "            end_pos = min(len(self.sentences), position + context_sentences + 1)\n",
    "\n",
    "            # Get context sentences\n",
    "            context = self.sentences[start_pos:end_pos]\n",
    "\n",
    "            # Mark which sentence is the actual match\n",
    "            is_highlight = [False] * len(context)\n",
    "            highlight_pos = position - start_pos\n",
    "            if 0 <= highlight_pos < len(context):\n",
    "                is_highlight[highlight_pos] = True\n",
    "\n",
    "            highlights.append({\n",
    "                'sentences': context,\n",
    "                'is_highlight': is_highlight,\n",
    "                'score': result['score'],\n",
    "                'start_position': start_pos,\n",
    "                'end_position': end_pos - 1\n",
    "            })\n",
    "\n",
    "        return highlights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(transcript_text: str, user_query: str) -> Tuple[List[Dict], List[Dict]]:\n",
    "    \"\"\"\n",
    "    Main function to rank and highlight transcript sentences based on a user query.\n",
    "\n",
    "    Args:\n",
    "        transcript_text: The transcript text from YouTube\n",
    "        user_query: The user's query\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (ranked_results, highlighted_transcript)\n",
    "    \"\"\"\n",
    "    # Initialize the ranker\n",
    "    ranker = TranscriptRanker()\n",
    "\n",
    "    # Process the transcript\n",
    "    ranker.prepare_transcript(transcript_text)\n",
    "\n",
    "    # Query for relevant sentences\n",
    "    ranked_results = ranker.query(user_query, top_k=5, diversity_factor=0.7)\n",
    "\n",
    "    # Get highlighted transcript segments\n",
    "    highlighted_transcript = ranker.highlight_transcript(ranked_results, context_sentences=1)\n",
    "\n",
    "    return ranked_results, highlighted_transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Programming projects\\Python\\Extractive Retrival Summarization\\py-venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ENVY 13TH GEN\\.cache\\huggingface\\hub\\models--sentence-transformers--all-mpnet-base-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top ranked sentences:\n",
      "1. Score: 0.6950 - Reinforcement learning teaches agents to make decisions by rewarding good actions.\n",
      "2. Score: 0.3259 - Deep learning, a subset of machine learning, uses neural\n",
      "    networks with many layers to process data.\n",
      "3. Score: 0.3246 - Explainable AI aims to make model decisions understandable to humans.\n",
      "4. Score: 0.3215 - It powers recommendation systems,\n",
      "    voice assistants, and autonomous vehicles.\n",
      "5. Score: 0.2805 - Computer vision enables\n",
      "    machines to interpret and make decisions based on visual data.\n",
      "\n",
      "Highlighted transcript segments:\n",
      "Segment 1 (Score: 0.6950):\n",
      "    Transfer learning allows models trained on one task to be\n",
      "    fine-tuned for another.\n",
      ">>> Reinforcement learning teaches agents to make decisions by rewarding good actions.\n",
      "    Natural language processing helps computers understand and generate human language.\n",
      "\n",
      "Segment 2 (Score: 0.3259):\n",
      "    It powers recommendation systems,\n",
      "    voice assistants, and autonomous vehicles.\n",
      ">>> Deep learning, a subset of machine learning, uses neural\n",
      "    networks with many layers to process data.\n",
      "    Transfer learning allows models trained on one task to be\n",
      "    fine-tuned for another.\n",
      "\n",
      "Segment 3 (Score: 0.3246):\n",
      "    Ethical considerations in AI include fairness, transparency, and privacy.\n",
      ">>> Explainable AI aims to make model decisions understandable to humans.\n",
      "\n",
      "Segment 4 (Score: 0.3215):\n",
      "    \n",
      "    Machine learning has become an essential part of modern technology.\n",
      ">>> It powers recommendation systems,\n",
      "    voice assistants, and autonomous vehicles.\n",
      "    Deep learning, a subset of machine learning, uses neural\n",
      "    networks with many layers to process data.\n",
      "\n",
      "Segment 5 (Score: 0.2805):\n",
      "    Natural language processing helps computers understand and generate human language.\n",
      ">>> Computer vision enables\n",
      "    machines to interpret and make decisions based on visual data.\n",
      "    Generative AI can create new content like\n",
      "    images, text, and music.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Example transcript (in practice, this would come from YouTube API)\n",
    "    sample_transcript = \"\"\"\n",
    "    Machine learning has become an essential part of modern technology. It powers recommendation systems,\n",
    "    voice assistants, and autonomous vehicles. Deep learning, a subset of machine learning, uses neural\n",
    "    networks with many layers to process data. Transfer learning allows models trained on one task to be\n",
    "    fine-tuned for another. Reinforcement learning teaches agents to make decisions by rewarding good actions.\n",
    "    Natural language processing helps computers understand and generate human language. Computer vision enables\n",
    "    machines to interpret and make decisions based on visual data. Generative AI can create new content like\n",
    "    images, text, and music. Ethical considerations in AI include fairness, transparency, and privacy.\n",
    "    Explainable AI aims to make model decisions understandable to humans.\n",
    "    \"\"\"\n",
    "\n",
    "    # Example query\n",
    "    query = \"How does reinforcement learning work?\"\n",
    "\n",
    "    # Process the query and transcript\n",
    "    ranked_results, highlighted_transcript = main(sample_transcript, query)\n",
    "\n",
    "    # Print results\n",
    "    print(\"Top ranked sentences:\")\n",
    "    for i, result in enumerate(ranked_results):\n",
    "        print(f\"{i+1}. Score: {result['score']:.4f} - {result['sentence']}\")\n",
    "\n",
    "    print(\"\\nHighlighted transcript segments:\")\n",
    "    for i, segment in enumerate(highlighted_transcript):\n",
    "        print(f\"Segment {i+1} (Score: {segment['score']:.4f}):\")\n",
    "        for j, (sentence, is_highlight) in enumerate(zip(segment['sentences'], segment['is_highlight'])):\n",
    "            if is_highlight:\n",
    "                print(f\">>> {sentence}\")\n",
    "            else:\n",
    "                print(f\"    {sentence}\")\n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
